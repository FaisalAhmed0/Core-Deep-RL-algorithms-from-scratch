{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code implements the REINFORCE algorithm for continuous action spaces using a gaussian policy\n",
    "![ ](images/reinforce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The REINFORCE Algorithm is one of the most Policy Gradient algorithms in renforcement learning\n",
    "Policy gradient algorithms directly diffierintate the RL objective \n",
    "$$ J(\\theta) = \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}{[\\sum_{t=1}^T}r(s_t,a_t)] $$\n",
    "from the definitation of Expectation \n",
    "$$J(\\theta) = \\int \\pi_\\theta(a_t|s_t) \\sum_{t=1}^Tr(s_t,a_t)  {d}(P(a_t|s_t) $$\n",
    "This objective can be defferinated to be \n",
    "$$\\nabla_\\theta J(\\theta) =\\int \\nabla_\\theta \\pi_\\theta(a_t|s_t) \\sum_{t=1}^Tr(s_t,a_t) {d}(P(a_t|s_t) $$\n",
    "We know from calculus that \n",
    "$${d}\\log{n} = \\frac{{d}n}{n}$$\n",
    "Hence the policy gradient is\n",
    "$$\\nabla_\\theta J(\\theta) =\\int  \\pi_\\theta(a_t|s_t) \\nabla_\\theta \\log {\\pi_\\theta(a_t|s_t)} \\sum_{t=1}^Tr(s_t,a_t) {d}(P(a_t|s_t) $$\n",
    "Which can be written as Expectation\n",
    "$$\\nabla_\\theta J(\\theta)  = \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}[\\nabla_\\theta \\log {\\pi_\\theta(a_t|s_t)}\\sum_{t=1}^Tr(s_t,a_t)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a practicial point of view the general structre of any Reinfrocement learning algorithm is shown below\n",
    "![image.png](images/Anatomy.png)\n",
    "for this vareint of REINFORCE algorithms the implementation structure will be based on the framework shown above  </br >  \n",
    "1. Generate Data by running the most recent policy, specifically this step should return states, rewards, and actions for each batch of training episode\n",
    "2. Return estimation will implement Monte Carlo policy evaluation with discount factos causality \n",
    "$$ \\hat{Q}(s_t,a_t) = \\sum_{{t'}=t}^T r(s_{t'},a_{t'})$$\n",
    "3. In this step the gradient ascent will be performed on the policy after sampling the gradient and taking the <br>\n",
    "of the sample <br>\n",
    "$$\\theta = \\theta + \\alpha\\nabla{J(\\theta}) $$ where J is the RL objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import numpy.random as random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Network\n",
    "class Policy_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_Network, self).__init__()\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=64)\n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        # mean head\n",
    "        self.mu = nn.Linear(in_features=32, out_features=1)\n",
    "        # variance head\n",
    "        self.var = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # shared layers\n",
    "        # layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        \n",
    "        return self.mu(x), F.softplus(self.var(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole problem with discrete actions\n",
    "# Network archetacture\n",
    "\n",
    "class SampleGeneration():\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(network, env='MountainCarContinuous-v0', N=4):\n",
    "        states_N = []\n",
    "        actions_N = []\n",
    "        rewards_N = []\n",
    "        \n",
    "        env = gym.make(env)\n",
    "        for trajectory in range(N):\n",
    "            state = env.reset()\n",
    "            states_N.append(state)\n",
    "            rewards = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_t = torch.tensor(state.astype(np.float32)).unsqueeze(0)\n",
    "                # forward pass to generate mu and var\n",
    "                mu_t, var_t = network(state_t)\n",
    "                mu = mu_t.detach().item()\n",
    "                var = var_t.detach().item()\n",
    "                std = np.sqrt(var)\n",
    "                # sample an action from a normal distribution policy based on mu and var\n",
    "                action = random.normal(mu, std)\n",
    "                np.clip(action,-1,1)\n",
    "                actions_N.append(action)\n",
    "                state, reward, done, _ = env.step(np.array([action]))\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states_N.append(state)\n",
    "            rewards_N.append(np.array(rewards))\n",
    "            state_stack = np.stack(states_N)\n",
    "        return (state_stack, rewards_N, np.array(actions_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def list_to_torch_tensor(List):\n",
    "    l = []\n",
    "    for element in List:\n",
    "        for r in element:\n",
    "            l.append(r)\n",
    "    return torch.tensor(l, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnEstimator():\n",
    "    # reward_to_go\n",
    "    @staticmethod\n",
    "    def estimate_return(rewards):\n",
    "        gamma = 0.99\n",
    "        res = [[] for i in range(len(rewards))]\n",
    "        for i in range(len(rewards)):\n",
    "            sum_r = 0.0\n",
    "            for r in rewards[i]:\n",
    "                sum_r *= gamma\n",
    "                sum_r += r\n",
    "                res[i].append(sum_r)\n",
    "            res[i].reverse()\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(network, states, rewards_to_go, actions,optimizer, tb, step):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    states_t = torch.tensor(states, dtype=torch.float32)\n",
    "    q_t = list_to_torch_tensor(rewards_to_go)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.float32)\n",
    "    \n",
    "    mu_t, var_t = network(states_t) # mean\n",
    "    actions_log_probs_term1 = - (actions_t - mu_t)**2 / (2*var_t)\n",
    "    actions_log_probs_term2 = - torch.log(torch.sqrt(2 * np.pi * var_t  ))\n",
    "    actions_log_probs = actions_log_probs_term1 + actions_log_probs_term2\n",
    "    weighted_actions_log_probs = actions_log_probs * q_t\n",
    "#     loss_entropy = - 0.0001 * (torch.log(2 * np.pi * var_t) + 1).mean()\n",
    "    loss_policy =  - weighted_actions_log_probs.mean()\n",
    "    loss = loss_policy \n",
    "    tb.add_scalar('loss', loss, step)\n",
    "#     tb.add_scalar('weighted entropy', -loss_entropy, step)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_policy(network, env=\"MountainCarContinuous-v0\", render=False):\n",
    "    runs = 5\n",
    "    total_reward = 0.0\n",
    "    env = gym.make(env)\n",
    "    for run in range(runs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            state_t = torch.tensor(state, dtype=torch.float32)\n",
    "            action,_ = network(state_t)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "    env.close()\n",
    "    return total_reward / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy_Network(\n",
      "  (fc1): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (mu): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (var): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# policy imporvment\n",
    "# hyperparameters\n",
    "seeds = [1,4,10,42]\n",
    "episodes_num = 50000\n",
    "# batch size\n",
    "N = 10\n",
    "#######\n",
    "env = 'MountainCarContinuous-v0'\n",
    "rewards_100 = []\n",
    "lrs = 5e-5\n",
    "for lr in lrs:\n",
    "    policy = Policy_Network()\n",
    "    tb = SummaryWriter(comment=f\"-env={env}, lr={lr}\")\n",
    "    states = torch.tensor(gym.make(env).reset(), dtype=torch.float32)\n",
    "    tb.add_graph(policy, states)\n",
    "    print(policy)\n",
    "    optimizer = opt.Adam(policy.parameters(), lr=lr)\n",
    "    for i in range(episodes_num):\n",
    "        # run the policy\n",
    "        states, rewards, actions = SampleGeneration.generate_samples(policy, N=N)\n",
    "        # estimate the return\n",
    "        q = ReturnEstimator.estimate_return(rewards)\n",
    "        # imporove the polciy\n",
    "        improve_policy(policy, states, q, actions, optimizer, tb, i )\n",
    "        # test the policy\n",
    "        test_reward = test_policy(policy)\n",
    "        rewards_100.append(test_reward)\n",
    "        tb.add_scalar('reward', test_reward, i)\n",
    "        if len(rewards_100) >= 100:\n",
    "            reward_100 = sum(rewards_100) / 100.0\n",
    "            tb.add_scalar('reward_100', reward_100, i)\n",
    "            rewards_100 = []\n",
    "        for name, param in policy.named_parameters():\n",
    "            if name!='logstd':\n",
    "                tb.add_histogram(f'{name}', param, i)\n",
    "                tb.add_histogram(f'{name}.grad', param.grad, i)\n",
    "#                 tb.add_scalar(f'{name}.grad.mean', torch.mean(param.grad), i)\n",
    "#                 tb.add_scalar(f'{name}.grad.max', torch.max(param.grad), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics\n",
    "not successful \n",
    "![ ](images/training_dynamics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. CS 285 at UC Berkeley\n",
    "Deep Reinforcement Learning\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf\n",
    "2. Reinforcement Learning: An Introduction by Richard S. Sutton\n",
    "and Andrew G. Barto, chapter 13 http://incompleteideas.net/book/the-book-2nd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
