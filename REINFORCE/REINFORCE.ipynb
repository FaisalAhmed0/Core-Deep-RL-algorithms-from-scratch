{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code implements the REINFORCE algorithm\n",
    "![ ](images/reinforce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The REINFORCE Algorithm is simplestt Policy Gradient algorithms in deep renforcement learning <br>\n",
    "Policy gradient algorithms directly diffierintate the RL objective  \n",
    "$$ J(\\theta) = \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}{[\\sum_{t=1}^T}r(s_t,a_t)] $$\n",
    "from the definitation of Expectation \n",
    "$$J(\\theta) = \\int \\pi_\\theta(a_t|s_t) \\sum_{t=1}^Tr(s_t,a_t)  {d}(P(a_t|s_t) $$\n",
    "This objective can be defferinated to be \n",
    "$$\\nabla_\\theta J(\\theta) =\\int \\nabla_\\theta \\pi_\\theta(a_t|s_t) \\sum_{t=1}^Tr(s_t,a_t) {d}(P(a_t|s_t) $$\n",
    "We know from calculus that \n",
    "$${d}\\log{n} = \\frac{{d}n}{n}$$\n",
    "Hence the policy gradient is\n",
    "$$\\nabla_\\theta J(\\theta) =\\int  \\pi_\\theta(a_t|s_t) \\nabla_\\theta \\log {\\pi_\\theta(a_t|s_t)} \\sum_{t=1}^Tr(s_t,a_t) {d}(P(a_t|s_t) $$\n",
    "Which can be written as Expectation\n",
    "$$\\nabla_\\theta J(\\theta)  = \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}[\\nabla_\\theta \\log {\\pi_\\theta(a_t|s_t)}\\sum_{t=1}^Tr(s_t,a_t)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a practicial point of view the general structre of any Reinfrocement learning algorithm is shown below\n",
    "![image.png](images/Anatomy.png)\n",
    "for this vareint of REINFORCE algorithms the implementation structure will be based on the framework shown above  </br >  \n",
    "1. Generate Data by running the most recent policy, specifically this step should return states, rewards, and actions for each batch of training episode\n",
    "2. Return estimation will implement Monte Carlo policy evaluation with discount factos causality \n",
    "$$ \\hat{Q}(s_t,a_t) = \\sum_{{t'}=t}^T r(s_{t'},a_{t'})$$\n",
    "3. In this step the gradient ascent will be performed on the policy after sampling the gradient and taking the <br>\n",
    "of the sample <br>\n",
    "$$\\theta = \\theta + \\alpha\\nabla{J(\\theta}) $$ where J is the RL objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole problem with discrete actions\n",
    "# Network archetacture\n",
    "\n",
    "class SampleGeneration():\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(network, env='CartPole-v1', N=4):\n",
    "        states_N = []\n",
    "        actions_N = []\n",
    "        rewards_N = []\n",
    "        env = gym.make(env)\n",
    "        for trajectory in range(N):\n",
    "            state = env.reset()\n",
    "            states_N.append(state)\n",
    "            rewards = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_t = torch.tensor(state.astype(np.float32)).unsqueeze(0)\n",
    "                action_logits = network(state_t)\n",
    "                actions_prob = network.softmax(action_logits)\n",
    "                action = torch.multinomial(actions_prob, 1).item()\n",
    "                actions_N.append(action)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states_N.append(state)\n",
    "            rewards_N.append(np.array(rewards))\n",
    "            state_stack = np.stack(states_N)\n",
    "        return (state_stack, rewards_N, np.array(actions_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Network\n",
    "class Policy_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # output layer\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def softmax(self, logits):\n",
    "        return F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def list_to_torch_tensor(List):\n",
    "    l = []\n",
    "    for element in List:\n",
    "        for r in element:\n",
    "            l.append(r)\n",
    "    return torch.tensor(l, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnEstimator():\n",
    "    # reward_to_go\n",
    "    @staticmethod\n",
    "    def estimate_return(rewards):\n",
    "        gamma = 0.99\n",
    "        res = [[] for i in range(len(rewards))]\n",
    "        for i in range(len(rewards)):\n",
    "            sum_r = 0.0\n",
    "            for r in rewards[i]:\n",
    "                sum_r *= gamma\n",
    "                sum_r += r\n",
    "                res[i].append(sum_r)\n",
    "            res[i].reverse()\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(network, states, rewards_to_go, actions, optimizer, tb, step):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    states_t = torch.tensor(states, dtype=torch.float32)\n",
    "    q = list_to_torch_tensor(rewards_to_go)\n",
    "    q_t = torch.tensor(q, dtype=torch.float32)\n",
    "    \n",
    "    logits = network(states_t)\n",
    "    actions_log_probs = F.log_softmax(logits, dim=1)\n",
    "    selected_actions_log_probs_t = actions_log_probs[range(len(actions_log_probs)), actions]\n",
    "    loss = selected_actions_log_probs_t * q_t\n",
    "    loss = -loss.mean()\n",
    "    tb.add_scalar('loss', loss, step)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_policy(network, env=\"CartPole-v1\"):\n",
    "    runs = 5\n",
    "    total_reward = 0.0\n",
    "    env = gym.make(env)\n",
    "    for run in range(runs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.tensor(state, dtype=torch.float32)\n",
    "            action_logits = network(state_t)\n",
    "            action = action_logits.argmax(dim=0).item()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "    return total_reward / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# policy imporvment\n",
    "# hyperparameters\n",
    "episodes_num = 10000\n",
    "# batch size\n",
    "N = 4\n",
    "#######\n",
    "env = 'CartPole-v1'\n",
    "## track the test rewards of the last 100 runs\n",
    "rewards_100 = []\n",
    "lrs = 0.001\n",
    "for lr in lrs:\n",
    "    policy = Policy_Network()\n",
    "    tb = SummaryWriter(comment=f\"-lr={lr}\")\n",
    "    states = torch.tensor(gym.make(env).reset(), dtype=torch.float32)\n",
    "    # add the policy graph to tensorboard\n",
    "    tb.add_graph(policy, states)\n",
    "    print(policy)\n",
    "    # Adam optimizer was used you can change it with your favorite optimizer\n",
    "    optimizer = opt.Adam(policy.parameters(), lr=lr)\n",
    "    for i in range(episodes_num):\n",
    "        # run the policy and collect data\n",
    "        states, rewards, acttions = SampleGeneration.generate_samples(policy, env=env,N=N)\n",
    "        # estimate the return (Monte carlo evaluation)\n",
    "        q = ReturnEstimator.estimate_return(rewards)\n",
    "        # imporove the polciy by taking a step in the gradient ascent direction\n",
    "        improve_policy(policy, states, q, actions, optimizer, tb, i )\n",
    "        # test the policy by rinning the env 5 times and average the result\n",
    "        test_reward = test_policy(policy, env=env)\n",
    "        rewards_100.append(test_reward)\n",
    "        # write the reward to tensorboard\n",
    "        tb.add_scalar('reward', test_reward, i)\n",
    "\n",
    "        if len(rewards_100) >= 100:\n",
    "            reward_100 = sum(rewards_100) / 100.0\n",
    "            tb.add_scalar('reward_100', reward_100, i)\n",
    "            rewards_100 = []\n",
    "            # winning test\n",
    "            if reward_100 >= 450:\n",
    "                print(\"done\")\n",
    "                break\n",
    "        for name, param in policy.named_parameters():\n",
    "            tb.add_histogram(f'{name}', param, i)\n",
    "            tb.add_histogram(f'{name}.grad', param.grad, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Performence\n",
    "<video width=320 height=240 controls src=\"recording/test.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and comment\n",
    "1. The performance of the algorithm can be improved by using any baseline which is not a function of actions\n",
    "2. To reduce the variance of the policy gradient TD learning can be used to evaluate (But it is baised)\n",
    "3. due to my lack of computing power and inability to acess the cloud from my country I test the code on simple\n",
    " environemnt but you can modify it to work with GPUs it is easy.\n",
    "4. always check the gradient values to ensure the learning process is healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. CS 285 at UC Berkeley\n",
    "Deep Reinforcement Learning\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf\n",
    "2. Reinforcement Learning: An Introduction by Richard S. Sutton\n",
    "and Andrew G. Barto, chapter 13 http://incompleteideas.net/book/the-book-2nd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
