{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code implements the Batch Actor Critic Algorithm for continuous action spaces\n",
    "![image.png](images/A2C.png) \n",
    "![image.png](images/Anatomy.png)\n",
    "for this implementation is based on the structure shown above </br >  \n",
    "1. Generate Data by running the most recent policy, specifically this step should return states, rewards, and actions for each batch of training episode\n",
    "2. Return estimation by any method, in this case n-step TD \n",
    "3. In this step the gradient ascent will be performed on the policy after sampling the gradient and taking the <br>\n",
    "of the sample <br>\n",
    "$$\\theta = \\theta + \\alpha\\nabla{J(\\theta}) $$ where J is the RL objective<br>\n",
    "Entropy maximization was used to improve exploration and robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Network\n",
    "class Actor_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor_Network, self).__init__()\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=64)\n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        # mean head\n",
    "        self.mu = nn.Linear(in_features=32, out_features=1)\n",
    "        # variance head\n",
    "        self.var = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # shared layers\n",
    "        # layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        \n",
    "        return self.mu(x), F.softplus(self.var(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Network\n",
    "class Critic_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic_Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # layefile:///home/faisal/Documents/ML%20practice/Deep%20Learning/pytorch/REINFORCE/REINFORCE.ipynbr 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # layer 2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # output layer\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    def softmax(self, logits):\n",
    "        return F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole problem with discrete actions\n",
    "# Network archetacture\n",
    "\n",
    "class SampleGeneration():\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(network, env='CartPole-v0', N=4):\n",
    "        states_N = []\n",
    "        actions_N = []\n",
    "        rewards_N = []\n",
    "        env = gym.make(env)\n",
    "        for trajectory in range(N):\n",
    "            state = env.reset()\n",
    "            states_N.append(state)\n",
    "            rewards = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_t = torch.tensor(state.astype(np.float32)).unsqueeze(0)\n",
    "                action_logits = network(state_t)\n",
    "                actions_prob = network.softmax(action_logits)\n",
    "                action = torch.multinomial(actions_prob, 1).item()\n",
    "                actions_N.append(action)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states_N.append(state)\n",
    "            rewards_N.append(np.array(rewards))\n",
    "            state_stack = np.stack(states_N)\n",
    "        return (state_stack, rewards_N, np.array(actions_N))\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def generate_samples_continuous(network, env='MountainCarContinuous-v0', N=4):\n",
    "        states_N = []\n",
    "        actions_N = []\n",
    "        rewards_N = []\n",
    "        \n",
    "        env = gym.make(env)\n",
    "        for trajectory in range(N):\n",
    "            state = env.reset()\n",
    "            states_N.append(state)\n",
    "            rewards = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_t = torch.tensor(state.astype(np.float32)).unsqueeze(0)\n",
    "                # forward pass to generate mu and var\n",
    "                mu_t, var_t = network(state_t)\n",
    "                mu = mu_t.detach().item()\n",
    "                var = var_t.detach().item()\n",
    "                std = np.sqrt(var)\n",
    "                # sample an action from a normal distribution policy based on mu and var\n",
    "                action = random.normal(mu, std)\n",
    "                np.clip(action,-1,1)\n",
    "                actions_N.append(action)\n",
    "                state, reward, done, _ = env.step(np.array([action]))\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states_N.append(state)\n",
    "            rewards_N.append(np.array(rewards))\n",
    "            state_stack = np.stack(states_N)\n",
    "        return (state_stack, rewards_N, np.array(actions_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def list_to_torch_tensor(List):\n",
    "    l = []\n",
    "    for element in List:\n",
    "        for r in element:\n",
    "            l.append(r)\n",
    "    return torch.tensor(l, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def n_step_TD(net,states,rewards,n):\n",
    "    res = []\n",
    "    gamma = 0.99\n",
    "    T = len(rewards)\n",
    "    for t in range(T):\n",
    "        sum_r = 0\n",
    "        taw = t - n\n",
    "        if taw < 0:\n",
    "            res.append(rewards[t])\n",
    "        if taw >= 0:\n",
    "            for i in range(taw,min(taw+n,T)):\n",
    "                sum_r += (gamma**(i-taw) * rewards[i])\n",
    "            if (taw+n) < T:\n",
    "                sum_r += gamma**n * net(torch.tensor(states[taw+n], dtype=torch.float32)).item()\n",
    "            res.append(sum_r)\n",
    "    res.reverse()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnEstimator():\n",
    "    # reward_to_go\n",
    "    @staticmethod\n",
    "    def estimate_return(rewards):\n",
    "        gamma = 0.99\n",
    "        res = [[] for i in range(len(rewards))]\n",
    "        for i in range(len(rewards)):\n",
    "            sum_r = 0.0\n",
    "            for r in rewards[i]:\n",
    "                sum_r *= gamma\n",
    "                sum_r += r\n",
    "                res[i].append(sum_r)\n",
    "            res[i].reverse()\n",
    "        return np.array(res)\n",
    "    @staticmethod\n",
    "    def n_step_batach(net,states,rewards_n,n):\n",
    "        res = []\n",
    "        for i in range(len(rewards_n)):\n",
    "            if i == 0:\n",
    "                cur_states = states[i:len(rewards_n[i])]\n",
    "            else:\n",
    "                cur_states = states[len(rewards_n[i-1]):len(rewards_n[i])+len(rewards_n[i-1])]\n",
    "            res.append(n_step_TD(net,cur_states,rewards_n[i],n))\n",
    "        return np.array(res)\n",
    "    @staticmethod\n",
    "    def fit_v(net, states, targets, opt, tb, step):\n",
    "        states_t = torch.tensor(states,dtype=torch.float32)\n",
    "        targets_t = list_to_torch_tensor(targets)\n",
    "        opt.zero_grad()\n",
    "        preds = net(states_t)\n",
    "        loss = F.mse_loss(preds.squeeze(1), targets_t)\n",
    "        tb.add_scalar('val_loss', loss, step)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    @torch.no_grad()\n",
    "    def calc_adv(net, targets, states):\n",
    "        states_t = torch.FloatTensor(states)\n",
    "        targets_t = list_to_torch_tensor(targets)\n",
    "        values = net(states_t)\n",
    "        adv = targets_t - values\n",
    "        return adv\n",
    "    @torch.no_grad()\n",
    "    def calc_adv_from_rewards(value_net,rewards,states, gamma=0.99):\n",
    "        advs = []\n",
    "        for i in range(len(rewards)):\n",
    "            eps_len = len(rewards[i])\n",
    "            last_state_reward = rewards[i][-1]\n",
    "#             rewards_n = np.array(rewards[i][:-1], dtype=np.float32)\n",
    "            rewards_t = torch.tensor(np.array(rewards[i][:-1]), dtype=torch.float32)\n",
    "            if i == 0:\n",
    "                cur_states = states[i:len(rewards[i])]\n",
    "            else:\n",
    "                cur_states = states[len(rewards[i-1]):len(rewards[i])+len(rewards[i-1])]\n",
    "#             print(next_states_t.shape)\n",
    "            states_t = torch.FloatTensor(cur_states[0:eps_len-1])\n",
    "            next_states_t = torch.FloatTensor(cur_states[1:eps_len])\n",
    "            states_values_t = value_net(states_t)\n",
    "            next_states_values_t = value_net(next_states_t)\n",
    "            rewards_t = rewards_t.reshape(-1,1)\n",
    "            adv_t = (rewards_t + (gamma * next_states_values_t)) - states_values_t\n",
    "            print(adv_t.shape)\n",
    "            adv_n = adv_t.detach().numpy().tolist()\n",
    "            adv_n.append([last_state_reward])\n",
    "            advs.append(adv_n)\n",
    "        return np.array(advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(network, states, adv, actions, optimizer, tb, step):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    states_t = torch.tensor(states, dtype=torch.float32)\n",
    "    tb.add_scalar('adv',torch.mean(adv),step)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.float32)\n",
    "    \n",
    "    mu_t, var_t = network(states_t) # mean\n",
    "    actions_log_probs_term1 = - (actions_t - mu_t)**2 / (2*var_t)\n",
    "    actions_log_probs_term2 = - torch.log(torch.sqrt(2 * np.pi * var_t  ))\n",
    "    actions_log_probs = actions_log_probs_term1 + actions_log_probs_term2\n",
    "    weighted_actions_log_probs = actions_log_probs * adv\n",
    "    loss_policy =  - weighted_actions_log_probs.mean()\n",
    "    \n",
    "    entropy =  - (torch.log(2 * np.pi * var_t) + 1).mean()\n",
    "    loss_entropy = - 0.0001 * entropy\n",
    "    loss = loss_policy + loss_entropy\n",
    "    tb.add_scalar('loss', loss, step)\n",
    "    tb.add_scalar('entropy', entropy, step)\n",
    "    tb.add_scalar('entropy loss', loss_entropy, step)\n",
    "    tb.add_scalar('policy loss', loss_policy, step)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_policy(network, mean, std, env=\"MountainCarContinuous-v0\", render=False):\n",
    "    runs = 1\n",
    "    total_reward = 0.0\n",
    "    env = gym.make(env)\n",
    "    env = gym.wrappers.Monitor(env, \"recording\")\n",
    "    for run in range(runs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        for i in range(1000):\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = normalize(state, mean, std)\n",
    "            state_t = torch.tensor(state, dtype=torch.float32)\n",
    "            action,_ = network(state_t)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return total_reward / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Actor_Network()\n",
    "critic = Critic_Network()\n",
    "states, rewards, actions = SampleGeneration.generate_samples_continuous(policy, N=4)\n",
    "estomatore = ReturnEstimator(mean, std)\n",
    "# tb = SummaryWriter(comment=f\"-lr={lr}\")\n",
    "# tb.add_graph(policy, states)\n",
    "sum_rewards = ReturnEstimator.estimate_return(rewards)\n",
    "ReturnEstimator.fit_v(critic, states, sum_rewards, crt_optimizer, tb, i)\n",
    "ReturnEstimator.calc_adv_from_rewards(critic, rewards, states)\n",
    "len(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input normalization\n",
    "def normalization_params(env=\"MountainCarContinuous-v0\"):\n",
    "    env = gym.make(env)\n",
    "    samples = []\n",
    "    for i in range(1000):\n",
    "        samples.append(env.observation_space.sample())\n",
    "    samples_np = np.array(samples)\n",
    "    return (samples_np.mean(axis=0), samples_np.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = normalization_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(state, mean, std):\n",
    "    return (state - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# policy imporvment\n",
    "# hyperparameters\n",
    "seeds = [1,4,10,42]\n",
    "episodes_num = 30000\n",
    "N = 8\n",
    "save_path = '/home/faisal/Documents/ML practice/Deep Learning/pytorch/Contunous_control/policy.pt'\n",
    "#######\n",
    "env = \"MountainCarContinuous-v0\"\n",
    "rewards_100 = []\n",
    "lrs = [0.0003, 0.00001, 0.000001]\n",
    "for lr in lrs:\n",
    "    policy = Actor_Network()\n",
    "    critic = Critic_Network()\n",
    "    tb = SummaryWriter(comment=f\"-lr={lr}\")\n",
    "    states = torch.tensor(gym.make(env).reset(), dtype=torch.float32)\n",
    "    tb.add_graph(policy, states)\n",
    "    print(policy)\n",
    "    act_optimizer = opt.Adam(policy.parameters(), lr=lr)\n",
    "    crt_optimizer = opt.Adam(critic.parameters(), lr=lr)\n",
    "    c = 0\n",
    "    test_reward = 0\n",
    "    for i in range(episodes_num):\n",
    "        # run the policy\n",
    "        if test_reward > 80 and c < 1:\n",
    "            lr = 5e-5\n",
    "            print('lr changed to 5e-5')\n",
    "            act_optimizer = opt.Adam(policy.parameters(), lr=lr)\n",
    "            crt_optimizer = opt.Adam(critic.parameters(), lr=lr)\n",
    "            c = 1\n",
    "        states, rewards, actions = SampleGeneration.generate_samples_continuous(policy, N=N, env=env)\n",
    "        states = normalize(states, mean, std)\n",
    "        # estimate the return\n",
    "        td_targets = ReturnEstimator.n_step_batach(critic, states, rewards,6)\n",
    "#         sum_rewards = ReturnEstimator.estimate_return(rewards)\n",
    "        ReturnEstimator.fit_v(critic, states, td_targets, crt_optimizer, tb, i)\n",
    "#         adv = ReturnEstimator.calc_adv(critic, rewards, states)\n",
    "#         adv = list_to_torch_tensor(adv)\n",
    "        adv = ReturnEstimator.calc_adv(critic, td_targets, states)\n",
    "        # imporove the polciy\n",
    "        improve_policy(policy, states, adv, actions, act_optimizer, tb, i )\n",
    "        # test the policy\n",
    "        test_reward = test_policy(policy, mean, std, env=env)\n",
    "        rewards_100.append(test_reward)\n",
    "        tb.add_scalar('reward', test_reward, i)\n",
    "\n",
    "        if len(rewards_100) >= 100:\n",
    "            reward_100 = sum(rewards_100) / 100.0\n",
    "            tb.add_scalar('reward_100', reward_100, i)\n",
    "            if reward_100 > 90:\n",
    "                torch.save(policy.state_dict(), save_path)\n",
    "            rewards_100 = []\n",
    "        for name, param in policy.named_parameters():\n",
    "            tb.add_histogram(f'{name}', param, i)\n",
    "            tb.add_histogram(f'{name}.grad', param.grad, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics\n",
    "<img  src=\"images/adv_loss.png\" > \n",
    "<img  src=\"images/reward_val.png\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Performence \n",
    "<video  controls width=\"320\" height=\"240\" src=\"recording/test.mp4\" type=\"video/mp4\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. CS 285 at UC Berkeley\n",
    "Deep Reinforcement Learning\n",
    "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf\n",
    "2. Reinforcement Learning: An Introduction by Richard S. Sutton\n",
    "and Andrew G. Barto, chapter 13 http://incompleteideas.net/book/the-book-2nd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
