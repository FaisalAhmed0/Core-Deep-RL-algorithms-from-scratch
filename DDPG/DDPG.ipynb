{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook implements the DDPG algorithm for continuous control \n",
    "![](images/DDPG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_name = \"HopperBulletEnv-v0\"\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.shape)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_shape, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64 + n_actions , out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "    def _flat_conv_size(self, image_shape):\n",
    "        zeros = torch.zeros(image_shape)\n",
    "        x = self.conv1(zeros.unsqueeze(0))\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        size = torch.prod(torch.tensor(x.shape)).data.item()\n",
    "        return size\n",
    "    \n",
    "    def forward(self,x, a):\n",
    "        # dense layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # dense layer 2\n",
    "        if (type(a) == float):\n",
    "          a = torch.tensor([[a]])\n",
    "          x = F.relu(self.fc2(torch.cat([x, a], dim=1)))\n",
    "        else:\n",
    "          x = F.relu(self.fc2(torch.cat([x, a], dim=1)))\n",
    "        # output layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_shape, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=n_actions)\n",
    "        \n",
    "    def _flat_conv_size(self, image_shape):\n",
    "        zeros = torch.zeros(image_shape)\n",
    "        x = self.conv1(zeros.unsqueeze(0))\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        size = torch.prod(torch.tensor(x.shape)).data.item()\n",
    "        return size\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # dense layer 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # dense layer 2\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # output layer\n",
    "        x = torch.tanh(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.buffer = collections.deque(maxlen=length)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        indices = np.random.randint(low=0, high=self.length,size=size)\n",
    "        return np.array(self.buffer)[indices]\n",
    "    \n",
    "    def append(self, e):\n",
    "        self.buffer.append(e)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Replay Buffer \\n {}\".format(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleGeneration():\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(network, env='CartPole-v1', N=4):\n",
    "        states_N = []\n",
    "        actions_N = []\n",
    "        rewards_N = []\n",
    "        env = gym.make(env)\n",
    "        for trajectory in range(N):\n",
    "            state = env.reset()\n",
    "            states_N.append(state)\n",
    "            rewards = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_t = torch.tensor(state.astype(np.float32)).unsqueeze(0)\n",
    "                action_logits = network(state_t)\n",
    "                actions_prob = network.softmax(action_logits)\n",
    "                action = torch.multinomial(actions_prob, 1).item()\n",
    "                actions_N.append(action)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                if not done:\n",
    "                    states_N.append(state)\n",
    "            rewards_N.append(np.array(rewards))\n",
    "            state_stack = np.stack(states_N)\n",
    "        return (state_stack, rewards_N, np.array(actions_N))\n",
    "    @torch.no_grad()\n",
    "    def generate_samples_from_buffer(buffer, mini_batch_size):\n",
    "        return buffer.sample(mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnEstimator():\n",
    "    # reward_to_go\n",
    "    @staticmethod\n",
    "    def estimate_return(rewards):\n",
    "        gamma = 0.99\n",
    "        res = [[] for i in range(len(rewards))]\n",
    "        for i in range(len(rewards)):\n",
    "            sum_r = 0.0\n",
    "            for r in rewards[i]:\n",
    "                sum_r *= gamma\n",
    "                sum_r += r\n",
    "                res[i].append(sum_r)\n",
    "            res[i].reverse()\n",
    "        return np.array(res)\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def calc_y(act_t, crt_t, states,rewards, n_states ,dones ,gamma=0.99):\n",
    "        states_t = torch.FloatTensor(states).to(torch.device(device))\n",
    "        n_states_t = torch.FloatTensor(n_states).to(torch.device(device))\n",
    "        rewards_t = torch.FloatTensor(rewards).to(torch.device(device))\n",
    "        \n",
    "        actions = act_t(n_states_t)\n",
    "        q_next_state = crt_t(n_states_t, actions)\n",
    "#         print('q shape',q_next_state.shape)\n",
    "        \n",
    "        done_mask = np.ones(rewards.shape)\n",
    "        done_mask[dones] = 0\n",
    "        done_mask_t = torch.FloatTensor(done_mask)\n",
    "        done_mask_t = done_mask_t.reshape(-1,1)\n",
    "#         print('done shape',done_mask_t.shape)\n",
    "        rewards_t = rewards_t.reshape(-1,1)\n",
    "#         print('rewards tensor shape',rewards_t.shape)\n",
    "        y = rewards_t + gamma * q_next_state * done_mask_t\n",
    "#         print('y shape',y.shape)\n",
    "#         print('y',y)\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def fit_Q(network, states, actions, y, optimizer, t):\n",
    "        optimizer.zero_grad()\n",
    "        states_t = torch.FloatTensor(states).to(torch.device(device))\n",
    "        actions_t = torch.FloatTensor(actions).to(torch.device(device))\n",
    "\n",
    "        Q_output = network(states_t, actions_t).to(torch.device(device))\n",
    "#         print('Q output',Q_output)\n",
    "#         print('y mean', y.mean())\n",
    "#         print(\"Q_output mean\",Q_output.mean())\n",
    "        loss = F.mse_loss(Q_output, y.detach())\n",
    "        tb.add_scalar('critic_loss', loss, t)\n",
    "        tb.add_scalar('Q mean', Q_output.mean(), t)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(act_network, crt_network,states, act_optimizer, t):\n",
    "    act_optimizer.zero_grad()\n",
    "    states_t = torch.FloatTensor(states).to(torch.device(device))\n",
    "    actions = act_network(states_t)\n",
    "#     actions = torch.clamp(actions, -1, 1)\n",
    "    loss = - crt_network(states_t, actions).to(torch.device(device)).mean()\n",
    "    loss.backward()\n",
    "    act_optimizer.step()\n",
    "    tb.add_scalar('actor_loss', loss, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU_process for exploration noise \n",
    "@torch.no_grad()\n",
    "def OU_process(mu_t, prev_noise, ou_mu = 0.0,theta=0.15, sigma=0.2, eps=1):\n",
    "    mu = mu_t.cpu().detach().numpy()\n",
    "    action_size = mu.shape[0]\n",
    "#     print(action_size)\n",
    "    ou_noise = prev_noise + theta*(ou_mu - prev_noise) + sigma * np.random.normal(size=action_size)\n",
    "    action = mu + eps * ou_noise\n",
    "    return action, ou_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def Play_step(network, env, obs, buffer,e, total_reward, prev_noise):\n",
    "    obs_t = torch.FloatTensor(obs).to(torch.device(device))\n",
    "    mu_t = network(obs_t)\n",
    "    if prev_noise is None:\n",
    "        prev_noise = np.zeros(shape=mu_t.shape[0])\n",
    "    action, prev_noise = OU_process(mu_t, prev_noise)\n",
    "#     action = action.clip(-1,1)\n",
    "    n_obs, reward, done, _ = env.step(action)\n",
    "    buffer.append(e(obs, action, reward, n_obs, done))\n",
    "    total_reward += reward\n",
    "    return n_obs, done, total_reward, prev_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_policy(network, env_name, render=False):\n",
    "    runs = 5\n",
    "    total_reward = 0.0\n",
    "    env = gym.make(env_name)\n",
    "    for run in range(runs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            state_t = torch.FloatTensor(state).to(torch.device(device))\n",
    "            action = network(state_t)\n",
    "            action = action.cpu().numpy()\n",
    "#             action = action.clip(-1,1)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "    env.close()\n",
    "    return total_reward / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_minibatch(buffer, sample_size):\n",
    "    batch = buffer.sample(sample_size)\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    n_states = []\n",
    "    dones = []\n",
    "    for e in batch:\n",
    "        states.append(e[0])\n",
    "        actions.append(e[1])\n",
    "        rewards.append(e[2])\n",
    "        n_states.append(e[3])\n",
    "        dones.append(e[4])\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(n_states), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48560227/how-to-take-the-average-of-the-weights-of-two-networks\n",
    "@torch.no_grad()\n",
    "def polyak(net, target_net, taw=0.001):\n",
    "    params1 = net.named_parameters()\n",
    "    params2 = target_net.named_parameters()\n",
    "\n",
    "    dict_params2 = dict(params2)\n",
    "\n",
    "    for name1, param1 in params1:\n",
    "        if name1 in dict_params2:\n",
    "            dict_params2[name1].data.copy_(taw*param1.data + (1-taw)*dict_params2[name1].data)\n",
    "\n",
    "    target_net.load_state_dict(dict_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "Buffer_size = 100000\n",
    "target_sync = 1000\n",
    "\n",
    "# Create the environment ,the network, and the optimizer\n",
    "env_name = 'HopperBulletEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "obs_space = env.observation_space.shape\n",
    "action_space = env.action_space.shape\n",
    "\n",
    "act = Actor(obs_space[0], action_space[0]).to(torch.device(device))\n",
    "crt = Critic(obs_space[0], action_space[0]).to(torch.device(device))\n",
    "\n",
    "act_target = Actor(obs_space[0], action_space[0]).to(torch.device(device))\n",
    "crt_target = Critic(obs_space[0], action_space[0]).to(torch.device(device))\n",
    "\n",
    "act_optimizer = opt.Adam(act.parameters(), lr=lr)\n",
    "crt_optimizer = opt.Adam(crt.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "buffer = ReplayBuffer(Buffer_size)\n",
    "\n",
    "experience = collections.namedtuple('Experience', ['obs', 'action', 'reward', 'next_obs', 'done'])\n",
    "\n",
    "t = 0\n",
    "rewards_100 = []\n",
    "# Writitng to the summary \n",
    "obs = env.reset()\n",
    "tb = SummaryWriter(comment=f\"-DDPG-env={env_name}-lr={lr}\")\n",
    "# Add the actor graph to the network \n",
    "tb.add_graph(act, torch.FloatTensor(obs).unsqueeze(0).to(torch.device(device)))\n",
    "reward = 0\n",
    "max_reward_100 = 0\n",
    "# Exploration noise\n",
    "noise = None\n",
    "eps_len = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    # print(t)\n",
    "    # Fill the replay buffer\n",
    "    while len(buffer) < Buffer_size:\n",
    "        obs = env.reset()\n",
    "        for i in range(1000):\n",
    "            n_obs, done, reward, noise = Play_step(act, env, obs, buffer, experience, reward, noise)\n",
    "            obs = n_obs\n",
    "            if done:\n",
    "                done = False\n",
    "                obs = env.reset()\n",
    "                reward = 0  \n",
    "                noise = None\n",
    "                break\n",
    "    obs, done, reward, noise = Play_step(act, env, obs, buffer, experience, reward, noise)\n",
    "    eps_len += 1\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        noise = None\n",
    "        # print(\"done\")\n",
    "        tb.add_scalar(\"eps steps\", eps_len, t)\n",
    "        tb.add_scalar(\"training reward\", reward, t)\n",
    "        eps_len = 0\n",
    "        # print(\"reward\",reward)\n",
    "        reward = 0\n",
    "        test_reward = test_policy(act, env_name)\n",
    "        rewards_100.append(test_reward)\n",
    "        tb.add_scalar(\"testing reward\", test_reward, t)\n",
    "        # Write the average reward of the last 100 episode to tensorboard\n",
    "        if len(rewards_100) == 100:\n",
    "            if (sum(rewards_100)/100) > max_reward_100:\n",
    "                torch.save(act.state_dict(),\"act_network.pt\")\n",
    "                max_reward_100 = sum(rewards_100)/100\n",
    "                print(\"new max reward = \",max_reward_100)\n",
    "            reward_100 = sum(rewards_100)/100\n",
    "            print(f\"{t}, testing_reward: {test_reward}, training_reward: {rewards_100[-1]}, mean_rewad: {reward_100}\")\n",
    "            tb.add_scalar('reward_100', reward_100, t)\n",
    "            rewards_100 = []\n",
    "            if reward_100 >= 800:\n",
    "                print(\"done\")\n",
    "                break\n",
    "\n",
    "    # check the buffer size\n",
    "    # print(len(buffer))\n",
    "    if len(buffer) < Buffer_size:\n",
    "        continue\n",
    "    # sample a batch\n",
    "    states, actions, rewards, n_states, dones = sample_random_minibatch(buffer, batch_size)\n",
    "#     states = normalize(states,mean,std)\n",
    "#     n_states = normalize(n_states,mean,std)\n",
    "    # calculate y\n",
    "    y = ReturnEstimator.calc_y(act_target, crt_target, states, rewards, n_states, dones)\n",
    "    # Update the Crtitc network \n",
    "    ReturnEstimator.fit_Q(crt, states, actions, y, crt_optimizer, t)\n",
    "    # update the actor network\n",
    "    improve_policy(act, crt, states, act_optimizer,t)\n",
    "    # update the target networks by applying polyak average\n",
    "    polyak(act, act_target, taw=0.001)\n",
    "    polyak(crt, crt_target, taw = 0.001)\n",
    "#     for name, param in act.named_parameters():\n",
    "#             tb.add_histogram(f'actor_{name}', param, t)\n",
    "#             tb.add_histogram(f'actor_{name}.grad', param.grad, t)\n",
    "#     for name, param in crt.named_parameters():\n",
    "#             tb.add_histogram(f'critic_{name}', param, t)\n",
    "#             tb.add_histogram(f'crirtc_{name}.grad', param.grad, t)\n",
    "    # update the target network by copying the weights\n",
    "#     if t%target_sync == 0:\n",
    "#         print(\"Target networks updated at step t = \", t)\n",
    "#         crt_target.load_state_dict(crt.state_dict())\n",
    "#         act_target.load_state_dict(act.state_dict())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics\n",
    "<img  src=\"images/Q_actor_loss.png\" > \n",
    "<img  src=\"images/critic_loss_eps_steps.png\" > \n",
    "<img  src=\"images/rewards.png\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Performence \n",
    "<video  controls width=\"320\" height=\"240\" src=\"recordings/test.mp4\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent and save the record the performence\n",
    "env = gym.wrappers.Monitor(gym.make(env_name),\"/home/faisal/Documents/ML practice/Deep Learning/pytorch/DDPG/recordings\")\n",
    "\n",
    "obs_space = env.observation_space.shape\n",
    "action_space = env.action_space.shape\n",
    "act = Actor(obs_space[0], action_space[0]).to(torch.device(device))\n",
    "act.load_state_dict(torch.load(\"act_network.pt\"))\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    state_t = torch.FloatTensor(state).to(torch.device(device))\n",
    "    action = act(state_t)\n",
    "    action = action.cpu().detach().numpy()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "Timothy P. Lillicrap et.al Continuous control with deep reinforcement learning https://arxiv.org/abs/1509.02971 <br>\n",
    "CS 285 at UC Berkeley\n",
    "Deep Reinforcement Learning lecture 8 http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
